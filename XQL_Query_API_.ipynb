{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: config - Edit the Values\n",
    "import time\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import os\n",
    "import gzip\n",
    "from datetime import datetime\n",
    "\n",
    "BASE_URL = \"YOUR_API_URL\"  # no trailing slash\n",
    "API_KEY = os.getenv(\"XDR_API_KEY\", \"YOUR_API_KEY\")\n",
    "API_KEY_ID = os.getenv(\"XDR_API_KEY_ID\", \"YOUR_API_KEY_ID\")\n",
    "# Time range for the hunt (last 30 days)\n",
    "DEFAULT_RELATIVE_TIME_MS = 30 * 24 * 60 * 60 * 1000\n",
    "# Global polling settings for long running XQL queries\n",
    "POLL_MAX_ATTEMPTS = 120        # times to poll before giving up\n",
    "POLL_SLEEP_SECONDS = 60       # wait time between polls\n",
    "POLL_MAX_ATTEMPT = POLL_MAX_ATTEMPTS\n",
    "\n",
    "\n",
    "\n",
    "# ===========================================================\n",
    "# Session with headers\n",
    "# ===========================================================\n",
    "SESSION = requests.Session()\n",
    "SESSION.headers.update({\n",
    "    \"Authorization\": API_KEY,\n",
    "    \"x-xdr-auth-id\": API_KEY_ID,\n",
    "    \"Accept-Encoding\": \"gzip\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "})\n",
    "def start_xql_query(query, relative_time_ms=DEFAULT_RELATIVE_TIME_MS, tenants=None):\n",
    "   \n",
    "   url = f\"{BASE_URL}/public_api/v1/xql/start_xql_query\"\n",
    "   timeframe = {\"relativeTime\": relative_time_ms}\n",
    "   request_data = {\"query\": query, \"timeframe\": timeframe}\n",
    "   if tenants is not None:\n",
    "       request_data[\"tenants\"] = tenants\n",
    "   payload = {\"request_data\": request_data}\n",
    "   resp = SESSION.post(url, data=json.dumps(payload))\n",
    "   resp.raise_for_status()\n",
    "   data = resp.json()\n",
    "   reply = data.get(\"reply\", data)\n",
    "   if isinstance(reply, dict):\n",
    "       return reply.get(\"query_id\") or reply.get(\"execution_id\")\n",
    "   return reply\n",
    "\n",
    "def get_query_results_basic(query_id, limit=1000000, pending_ok=False):\n",
    " \n",
    "   url = f\"{BASE_URL}/public_api/v1/xql/get_query_results\"\n",
    "   request_data = {\"query_id\": query_id, \"format\": \"json\"}\n",
    "   if limit is not None:\n",
    "       request_data[\"limit\"] = limit\n",
    "   payload = {\"request_data\": request_data}\n",
    "   resp = SESSION.post(url, data=json.dumps(payload))\n",
    "   resp.raise_for_status()\n",
    "   data = resp.json()\n",
    "   reply = data.get(\"reply\", data)\n",
    "   status = reply.get(\"status\", \"SUCCESS\")\n",
    "   results = reply.get(\"results\", {})\n",
    "   stream_id = results.get(\"stream_id\")\n",
    "   rows = []\n",
    "   if \"data\" in results:\n",
    "       rows = results[\"data\"]\n",
    "   elif \"data\" in reply:\n",
    "       rows = reply[\"data\"]\n",
    "   df = pd.DataFrame(rows)\n",
    "   if status != \"SUCCESS\" and not pending_ok:\n",
    "       raise ValueError(f\"Results not ready yet, status={status}\\n{json.dumps(data, indent=2)[:1000]}\")\n",
    "   return data, df, status, stream_id\n",
    "\n",
    "def download_stream_to_file(stream_id):\n",
    "  \n",
    "   \n",
    "   timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S_%f\")\n",
    "   base_dir = os.path.expanduser(f\"~/XQL_Results/{timestamp}\")\n",
    "   os.makedirs(base_dir, exist_ok=True)\n",
    "   gz_path = os.path.join(base_dir, \"results.json.gz\")\n",
    "   json_path = os.path.join(base_dir, \"results.json\")\n",
    "   # Build request\n",
    "   url = f\"{BASE_URL}/public_api/v1/xql/get_query_results_stream\"\n",
    "   headers = {\n",
    "       \"Accept\": \"application/json\",\n",
    "       \"Accept-Encoding\": \"gzip\",\n",
    "       \"Authorization\": API_KEY,\n",
    "       \"x-xdr-auth-id\": API_KEY_ID,\n",
    "       \"Content-Type\": \"application/json\"\n",
    "   }\n",
    "   body = {\n",
    "       \"request_data\": {\n",
    "           \"stream_id\": stream_id,\n",
    "           \"is_gzip_compressed\": True\n",
    "       }\n",
    "   }\n",
    "   \n",
    "   with requests.post(url, headers=headers, json=body, stream=True) as r:\n",
    "       r.raise_for_status()\n",
    "       # Save gzip file to disk\n",
    "       with open(gz_path, \"wb\") as f:\n",
    "           for chunk in r.iter_content(chunk_size=8192):\n",
    "               if chunk:\n",
    "                   f.write(chunk)\n",
    "   print(f\"[OK] Gzip stream saved to: {gz_path}\")\n",
    "   with gzip.open(gz_path, \"rb\") as gz_file:\n",
    "       data = gz_file.read()\n",
    "   with open(json_path, \"wb\") as out_json:\n",
    "       out_json.write(data)\n",
    "   print(f\"[OK] Extracted JSON to: {json_path}\")\n",
    "   return json_path\n",
    "\n",
    "def load_json_rows(json_path):\n",
    "  \n",
    "   rows = []\n",
    "   with open(json_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "       for line in f:\n",
    "           line = line.strip()\n",
    "           if not line:\n",
    "               continue\n",
    "           if line in [\"[\", \"]\", \"{\", \"}\", \",\"]:\n",
    "               continue\n",
    "           try:\n",
    "               obj = json.loads(line)\n",
    "               rows.append(obj)\n",
    "           except json.JSONDecodeError:\n",
    "               continue\n",
    "   print(f\"[OK] Loaded {len(rows)} rows from JSON\")\n",
    "   return pd.DataFrame(rows)\n",
    "\n",
    "def run_xql_auto_download(query):\n",
    " \n",
    "   query_id = start_xql_query(query)\n",
    "   print(f\"[run_xql_auto] Started query_id={query_id}\")\n",
    "   # poll until SUCCESS\n",
    "   for attempt in range(POLL_MAX_ATTEMPTS):\n",
    "       data, df_basic, status, stream_id = get_query_results_basic(\n",
    "           query_id=query_id,\n",
    "           limit=1000000,\n",
    "           pending_ok=True\n",
    "       )\n",
    "       print(f\"[poll] attempt={attempt+1}, status={status}, stream_id={stream_id}\")\n",
    "       if status == \"SUCCESS\":\n",
    "           break\n",
    "       time.sleep(POLL_SLEEP_SECONDS)\n",
    "   else:\n",
    "       raise RuntimeError(f\"Timed out polling XQL results for query {query_id}\")\n",
    "   \n",
    "   if stream_id:\n",
    "       print(f\"[run_xql_auto] Large result set â€“ downloading stream {stream_id}\")\n",
    "       json_path = download_stream_to_file(stream_id)\n",
    "\n",
    "       global LAST_STREAM_JSON_PATH\n",
    "       LAST_STREAM_JSON_PATH = json_path\n",
    "       df = load_json_rows(json_path)\n",
    "       print(f\"[run_xql_auto] Final row count: {len(df)}\")\n",
    "       return df, LAST_STREAM_JSON_PATH\n",
    "   \n",
    "   print(f\"[run_xql_auto] Using basic results: {len(df_basic)} rows\")\n",
    "   return df_basic, None\n",
    "\n",
    "# ===========================================================\n",
    "# XQL Query CELL\n",
    "# ===========================================================\n",
    "xql = \"\"\"\n",
    "#<Your XQL Query>\n",
    "\"\"\"\n",
    "df, _ = run_xql_auto_download(xql)\n",
    "print(f\"Total rows returned (auto): {len(df)}\")\n",
    "df.head(20)        # only print first 20 rows \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
